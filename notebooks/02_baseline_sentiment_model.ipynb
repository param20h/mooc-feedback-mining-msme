{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df946ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from data_preprocessing import TextPreprocessor, load_data\n",
    "from feature_extraction import FeatureExtractor\n",
    "from train_model import SentimentModel, prepare_train_test_split, train_multiple_models\n",
    "from evaluate_model import ModelEvaluator, compare_models\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… All modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1061a5",
   "metadata": {},
   "source": [
    "## 1. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768686d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "df = load_data('../data/raw/coursera_reviews.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0368bd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = TextPreprocessor(remove_stopwords=True, lemmatize=True)\n",
    "\n",
    "# Preprocess data\n",
    "# Adjust 'review' to your actual text column name\n",
    "df = preprocessor.preprocess_dataframe(df, text_column='review', output_column='cleaned_review')\n",
    "\n",
    "print(\"\\nâœ… Preprocessing completed!\")\n",
    "print(f\"Sample cleaned review:\\n{df['cleaned_review'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d28bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data\n",
    "df.to_csv('../data/processed/cleaned_reviews.csv', index=False)\n",
    "print(\"âœ… Processed data saved to data/processed/cleaned_reviews.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad41209",
   "metadata": {},
   "source": [
    "## 2. Feature Extraction (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c5bb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature extractor\n",
    "extractor = FeatureExtractor(method='tfidf', max_features=5000, ngram_range=(1, 2))\n",
    "\n",
    "# Extract features\n",
    "X = extractor.fit_transform(df['cleaned_review'])\n",
    "y = df['sentiment']  # Adjust to your actual label column\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "\n",
    "# Save vectorizer\n",
    "extractor.save_vectorizer('../models/tfidf_vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9049ecf2",
   "metadata": {},
   "source": [
    "## 3. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab857b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = prepare_train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad76d5f1",
   "metadata": {},
   "source": [
    "## 4. Train Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf8bcec",
   "metadata": {},
   "source": [
    "### 4.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8b1851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression\n",
    "lr_model = SentimentModel(model_type='logistic')\n",
    "lr_model.train(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "lr_evaluator = ModelEvaluator(lr_model.model, X_test, y_test, class_names=['Negative', 'Neutral', 'Positive'])\n",
    "lr_metrics = lr_evaluator.print_metrics()\n",
    "lr_evaluator.print_classification_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741929d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "lr_evaluator.plot_confusion_matrix(save_path='../reports/figures/confusion_matrix_lr.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1116caf0",
   "metadata": {},
   "source": [
    "### 4.2 Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b436831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Naive Bayes\n",
    "nb_model = SentimentModel(model_type='naive_bayes')\n",
    "nb_model.train(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "nb_evaluator = ModelEvaluator(nb_model.model, X_test, y_test, class_names=['Negative', 'Neutral', 'Positive'])\n",
    "nb_metrics = nb_evaluator.print_metrics()\n",
    "nb_evaluator.print_classification_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430c099c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "nb_evaluator.plot_confusion_matrix(save_path='../reports/figures/confusion_matrix_nb.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f7919b",
   "metadata": {},
   "source": [
    "### 4.3 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0f4d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest\n",
    "rf_model = SentimentModel(model_type='random_forest')\n",
    "rf_model.train(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "rf_evaluator = ModelEvaluator(rf_model.model, X_test, y_test, class_names=['Negative', 'Neutral', 'Positive'])\n",
    "rf_metrics = rf_evaluator.print_metrics()\n",
    "rf_evaluator.print_classification_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af664753",
   "metadata": {},
   "source": [
    "## 5. Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344cd236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "results = {\n",
    "    'Logistic Regression': {\n",
    "        'model': lr_model,\n",
    "        'train_accuracy': lr_model.model.score(X_train, y_train),\n",
    "        'test_accuracy': lr_metrics['accuracy']\n",
    "    },\n",
    "    'Naive Bayes': {\n",
    "        'model': nb_model,\n",
    "        'train_accuracy': nb_model.model.score(X_train, y_train),\n",
    "        'test_accuracy': nb_metrics['accuracy']\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'model': rf_model,\n",
    "        'train_accuracy': rf_model.model.score(X_train, y_train),\n",
    "        'test_accuracy': rf_metrics['accuracy']\n",
    "    }\n",
    "}\n",
    "\n",
    "comparison = compare_models(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500f0197",
   "metadata": {},
   "source": [
    "## 6. Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eafd400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best performing model\n",
    "best_model_name = comparison['Test Accuracy'].idxmax()\n",
    "best_model = results[best_model_name]['model']\n",
    "\n",
    "best_model.save_model('../models/best_baseline_model.pkl')\n",
    "print(f\"\\nâœ… Best model ({best_model_name}) saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d2f785",
   "metadata": {},
   "source": [
    "## 7. Test Predictions on Sample Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17aea0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on sample reviews\n",
    "sample_reviews = [\n",
    "    \"This course is absolutely amazing! I learned so much.\",\n",
    "    \"Terrible experience, waste of time and money.\",\n",
    "    \"It was okay, nothing special but not bad either.\"\n",
    "]\n",
    "\n",
    "# Preprocess\n",
    "cleaned_samples = [preprocessor.preprocess(review) for review in sample_reviews]\n",
    "\n",
    "# Transform to features\n",
    "sample_features = extractor.transform(cleaned_samples)\n",
    "\n",
    "# Predict\n",
    "predictions = best_model.predict(sample_features)\n",
    "probabilities = best_model.predict_proba(sample_features)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nðŸ”® Sample Predictions:\\n\")\n",
    "for i, review in enumerate(sample_reviews):\n",
    "    print(f\"Review: {review}\")\n",
    "    print(f\"Predicted Sentiment: {predictions[i]}\")\n",
    "    print(f\"Confidence: {probabilities[i].max():.2%}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a68ee8",
   "metadata": {},
   "source": [
    "## 8. Summary & Next Steps\n",
    "\n",
    "**Results:**\n",
    "- Best model: [Model name]\n",
    "- Test accuracy: [X%]\n",
    "- Key findings: [List]\n",
    "\n",
    "**Next Steps:**\n",
    "1. Hyperparameter tuning (Grid Search / Random Search)\n",
    "2. Try different feature representations (Word2Vec, FastText)\n",
    "3. Experiment with deep learning models (LSTM, BERT)\n",
    "4. Implement aspect-based sentiment analysis"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
