{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df946ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\param\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All modules imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from data_preprocessing import TextPreprocessor, load_data\n",
    "from feature_extraction import FeatureExtractor\n",
    "from train_model import SentimentModel, prepare_train_test_split, train_multiple_models\n",
    "from evaluate_model import ModelEvaluator, compare_models\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… All modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1061a5",
   "metadata": {},
   "source": [
    "## 1. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "768686d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 140320 reviews from ../data/raw/coursera_reviews.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CourseId</th>\n",
       "      <th>Review</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2-speed-it</td>\n",
       "      <td>BOring</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2-speed-it</td>\n",
       "      <td>Bravo !</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2-speed-it</td>\n",
       "      <td>Very goo</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2-speed-it</td>\n",
       "      <td>Great course - I recommend it for all, especia...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2-speed-it</td>\n",
       "      <td>One of the most useful course on IT Management!</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     CourseId                                             Review  Label\n",
       "0  2-speed-it                                             BOring      1\n",
       "1  2-speed-it                                            Bravo !      5\n",
       "2  2-speed-it                                           Very goo      5\n",
       "3  2-speed-it  Great course - I recommend it for all, especia...      5\n",
       "4  2-speed-it    One of the most useful course on IT Management!      5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load raw data\n",
    "df = load_data('../data/raw/coursera_reviews.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0368bd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Preprocessing completed!\n",
      "Sample cleaned review:\n",
      "boring\n"
     ]
    }
   ],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = TextPreprocessor(remove_stopwords=True, lemmatize=True)\n",
    "\n",
    "# Preprocess data\n",
    "# Adjust 'review' to your actual text column name\n",
    "df = preprocessor.preprocess_dataframe(df, text_column='Review', output_column='cleaned_review')\n",
    "\n",
    "print(\"\\nâœ… Preprocessing completed!\")\n",
    "print(f\"Sample cleaned review:\\n{df['cleaned_review'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66d28bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Processed data saved to data/processed/cleaned_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "# Save processed data\n",
    "df.to_csv('../data/processed/cleaned_reviews.csv', index=False)\n",
    "print(\"âœ… Processed data saved to data/processed/cleaned_reviews.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad41209",
   "metadata": {},
   "source": [
    "## 2. Feature Extraction (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87c5bb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Extracted 5000 features using TFIDF\n",
      "\n",
      "Feature matrix shape: (140320, 5000)\n",
      "Labels shape: (140320,)\n",
      "âœ… Vectorizer saved to ../models/tfidf_vectorizer.pkl\n"
     ]
    }
   ],
   "source": [
    "# Initialize feature extractor\n",
    "extractor = FeatureExtractor(method='tfidf', max_features=5000, ngram_range=(1, 2))\n",
    "\n",
    "# Extract features\n",
    "X = extractor.fit_transform(df['cleaned_review'])\n",
    "y = df['Label']  # Adjust to your actual label column\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "\n",
    "# Save vectorizer\n",
    "extractor.save_vectorizer('../models/tfidf_vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9049ecf2",
   "metadata": {},
   "source": [
    "## 3. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab857b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Data Split:\n",
      "   Training samples: 112256\n",
      "   Testing samples: 28064\n",
      "   Features: 5000\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = prepare_train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad76d5f1",
   "metadata": {},
   "source": [
    "## 4. Train Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf8bcec",
   "metadata": {},
   "source": [
    "### 4.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d8b1851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Training LOGISTIC model...\n",
      "âœ… Training completed!\n",
      "\n",
      "==================================================\n",
      "ðŸ“Š MODEL EVALUATION RESULTS\n",
      "==================================================\n",
      "Accuracy:  0.7885\n",
      "Precision: 0.7365\n",
      "Recall:    0.7885\n",
      "F1-Score:  0.7451\n",
      "==================================================\n",
      "\n",
      "ðŸ“‹ CLASSIFICATION REPORT:\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of classes, 5, does not match size of target_names, 3. Try specifying the labels parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m lr_evaluator \u001b[38;5;241m=\u001b[39m ModelEvaluator(lr_model\u001b[38;5;241m.\u001b[39mmodel, X_test, y_test, class_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNegative\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNeutral\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPositive\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      7\u001b[0m lr_metrics \u001b[38;5;241m=\u001b[39m lr_evaluator\u001b[38;5;241m.\u001b[39mprint_metrics()\n\u001b[1;32m----> 8\u001b[0m \u001b[43mlr_evaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_classification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mM:\\5th sem\\Ml2-project\\notebooks\\../src\\evaluate_model.py:94\u001b[0m, in \u001b[0;36mModelEvaluator.print_classification_report\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict()\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mðŸ“‹ CLASSIFICATION REPORT:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_names\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    212\u001b[0m         )\n\u001b[0;32m    213\u001b[0m     ):\n\u001b[1;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    224\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2567\u001b[0m, in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[0;32m   2561\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   2562\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels size, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m, does not match size of target_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2563\u001b[0m                 \u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names)\n\u001b[0;32m   2564\u001b[0m             )\n\u001b[0;32m   2565\u001b[0m         )\n\u001b[0;32m   2566\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2567\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2568\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of classes, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m, does not match size of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2569\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m. Try specifying the labels \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2570\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names))\n\u001b[0;32m   2571\u001b[0m         )\n\u001b[0;32m   2572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2573\u001b[0m     target_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m l \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m labels]\n",
      "\u001b[1;31mValueError\u001b[0m: Number of classes, 5, does not match size of target_names, 3. Try specifying the labels parameter"
     ]
    }
   ],
   "source": [
    "# Train Logistic Regression\n",
    "lr_model = SentimentModel(model_type='logistic')\n",
    "lr_model.train(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "lr_evaluator = ModelEvaluator(lr_model.model, X_test, y_test, class_names=['Negative', 'Neutral', 'Positive'])\n",
    "lr_metrics = lr_evaluator.print_metrics()\n",
    "lr_evaluator.print_classification_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741929d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "lr_evaluator.plot_confusion_matrix(save_path='../reports/figures/confusion_matrix_lr.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1116caf0",
   "metadata": {},
   "source": [
    "### 4.2 Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b436831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Naive Bayes\n",
    "nb_model = SentimentModel(model_type='naive_bayes')\n",
    "nb_model.train(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "nb_evaluator = ModelEvaluator(nb_model.model, X_test, y_test, class_names=['Negative', 'Neutral', 'Positive'])\n",
    "nb_metrics = nb_evaluator.print_metrics()\n",
    "nb_evaluator.print_classification_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430c099c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "nb_evaluator.plot_confusion_matrix(save_path='../reports/figures/confusion_matrix_nb.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f7919b",
   "metadata": {},
   "source": [
    "### 4.3 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0f4d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest\n",
    "rf_model = SentimentModel(model_type='random_forest')\n",
    "rf_model.train(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "rf_evaluator = ModelEvaluator(rf_model.model, X_test, y_test, class_names=['Negative', 'Neutral', 'Positive'])\n",
    "rf_metrics = rf_evaluator.print_metrics()\n",
    "rf_evaluator.print_classification_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af664753",
   "metadata": {},
   "source": [
    "## 5. Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344cd236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "results = {\n",
    "    'Logistic Regression': {\n",
    "        'model': lr_model,\n",
    "        'train_accuracy': lr_model.model.score(X_train, y_train),\n",
    "        'test_accuracy': lr_metrics['accuracy']\n",
    "    },\n",
    "    'Naive Bayes': {\n",
    "        'model': nb_model,\n",
    "        'train_accuracy': nb_model.model.score(X_train, y_train),\n",
    "        'test_accuracy': nb_metrics['accuracy']\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'model': rf_model,\n",
    "        'train_accuracy': rf_model.model.score(X_train, y_train),\n",
    "        'test_accuracy': rf_metrics['accuracy']\n",
    "    }\n",
    "}\n",
    "\n",
    "comparison = compare_models(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500f0197",
   "metadata": {},
   "source": [
    "## 6. Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eafd400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best performing model\n",
    "best_model_name = comparison['Test Accuracy'].idxmax()\n",
    "best_model = results[best_model_name]['model']\n",
    "\n",
    "best_model.save_model('../models/best_baseline_model.pkl')\n",
    "print(f\"\\nâœ… Best model ({best_model_name}) saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d2f785",
   "metadata": {},
   "source": [
    "## 7. Test Predictions on Sample Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17aea0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on sample reviews\n",
    "sample_reviews = [\n",
    "    \"This course is absolutely amazing! I learned so much.\",\n",
    "    \"Terrible experience, waste of time and money.\",\n",
    "    \"It was okay, nothing special but not bad either.\"\n",
    "]\n",
    "\n",
    "# Preprocess\n",
    "cleaned_samples = [preprocessor.preprocess(review) for review in sample_reviews]\n",
    "\n",
    "# Transform to features\n",
    "sample_features = extractor.transform(cleaned_samples)\n",
    "\n",
    "# Predict\n",
    "predictions = best_model.predict(sample_features)\n",
    "probabilities = best_model.predict_proba(sample_features)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nðŸ”® Sample Predictions:\\n\")\n",
    "for i, review in enumerate(sample_reviews):\n",
    "    print(f\"Review: {review}\")\n",
    "    print(f\"Predicted Sentiment: {predictions[i]}\")\n",
    "    print(f\"Confidence: {probabilities[i].max():.2%}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a68ee8",
   "metadata": {},
   "source": [
    "## 8. Summary & Next Steps\n",
    "\n",
    "**Results:**\n",
    "- Best model: [Model name]\n",
    "- Test accuracy: [X%]\n",
    "- Key findings: [List]\n",
    "\n",
    "**Next Steps:**\n",
    "1. Hyperparameter tuning (Grid Search / Random Search)\n",
    "2. Try different feature representations (Word2Vec, FastText)\n",
    "3. Experiment with deep learning models (LSTM, BERT)\n",
    "4. Implement aspect-based sentiment analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
